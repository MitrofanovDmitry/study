{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"id":"lzLqEeZKEEYz","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.decomposition import PCA\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"id":"KgnkkF5bEEY9","trusted":true},"cell_type":"code","source":"# Для вашего удобства списки с именами разных колонок\n\n# Числовые признаки\nnum_cols = [\n    'ClientPeriod',\n    'MonthlySpending',\n    'TotalSpent'\n]\n\n# Категориальные признаки\ncat_cols = [\n    'Sex',\n    'IsSeniorCitizen',\n    'HasPartner',\n    'HasChild',\n    'HasPhoneService',\n    'HasMultiplePhoneNumbers',\n    'HasInternetService',\n    'HasOnlineSecurityService',\n    'HasOnlineBackup',\n    'HasDeviceProtection',\n    'HasTechSupportAccess',\n    'HasOnlineTV',\n    'HasMovieSubscription',\n    'HasContractPhone',\n    'IsBillingPaperless',\n    'PaymentMethod'\n]\n\nfeature_cols = num_cols + cat_cols\ntarget_col = 'Churn'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Custom transformer which relaces spaces by zeros in TotalSpent column and\n# converts it to floats. Returns pd.DataFrame\nclass TotalSpentTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        # replace spaces by zeros\n        SImp = SimpleImputer(missing_values = \" \", fill_value = \"0\", strategy=\"constant\")\n        SImp.fit(X[[\"TotalSpent\"]])\n        new_TotalSpent = SImp.transform(X[[\"TotalSpent\"]])\n\n        # convert strings to float\n        new_TotalSpent = new_TotalSpent.astype(float)\n\n        # create new dataframe without changing input one\n        newX = X.assign(**{\"TotalSpent\" : new_TotalSpent})\n        return newX\n\ndef ClientPeriodTransformer(X):\n    # replace spaces by zeros\n    SImp = SimpleImputer(missing_values = 0, fill_value = 1, strategy=\"constant\")\n    SImp.fit(X[[\"ClientPeriod\"]])\n    new_ClientPeriod = SImp.transform(X[[\"ClientPeriod\"]])\n\n    new_TotalSpent = X[[\"TotalSpent\"]].copy()\n    new_TotalSpent.loc[X[\"TotalSpent\"] == 0, \"TotalSpent\"] = X[X[\"TotalSpent\"] == 0][\"MonthlySpending\"]\n\n    newX = X.assign(**{\"TotalSpent\" : new_TotalSpent, \"ClientPeriod\": new_ClientPeriod})\n    return newX\n\ndef add_spend_ratio(X):\n    newX = X.assign(**{\"spend_ratio\" : X[\"TotalSpent\"] / X[\"MonthlySpending\"]})\n    return newX\n\ndef add_monthly_avg(X):\n    newX = X.assign(**{\"MonthlyAvg\" : X[\"TotalSpent\"] / X[\"ClientPeriod\"]})\n    return newX\n\ndef add_last_more_than_avg(X):\n    newX = X.assign(**{\"LastMoreThanAvg\" : (X[\"MonthlySpending\"] / X[\"MonthlyAvg\"]).astype(int)})\n    return newX\n\ndef add_MonthlySpendingLog(X):\n    newX = X.assign(**{\"MonthlySpendingLog\" : np.log(X[\"MonthlySpending\"])})\n    return newX\n\ndef add_TotalSpentLog(X):\n    newX = X.assign(**{\"TotalSpentLog\" : np.log(X[\"TotalSpent\"])})\n    return newX\n\ndef remove_TotalSpent_MonthlySpending(X):\n    old_cols = X.columns\n    new_cols = [c for c in old_cols if c not in [\"TotalSpent\", \"MonthlySpending\"]]\n    newX = X[new_cols]\n    return newX\n\ndef myOneHotEncoderFunc(X, cat_cols=None):\n    ohe = OneHotEncoder(sparse=False)\n    encodedX = ohe.fit_transform(X[cat_cols])\n    encoded_cols = ohe.get_feature_names(cat_cols)\n    encodedX_df = pd.DataFrame(encodedX, columns=encoded_cols, index=X.index)\n    \n    non_cat_cols = [col for col in X.columns if col not in cat_cols]\n    \n    encoded_cols_filtered = []\n    for col in cat_cols:\n        encoded_cols_filtered.extend([c for c in encoded_cols if c.startswith(col + \"_\")][:-1])\n    newX = pd.concat([X[non_cat_cols], encodedX_df[encoded_cols_filtered]], axis=1)\n    return newX\n\ndef myOneHotEncoderTransformer(cat_cols):\n    return FunctionTransformer(myOneHotEncoderFunc, kw_args={\"cat_cols\":cat_cols})\n\ndef print_roc_auc_score(model, X, y):\n    roc_aucs = cross_val_score(model, X, y, cv=5, scoring=\"roc_auc\", n_jobs=-1)\n    print(f\"scores: {roc_aucs}\")\n    print(f\"avg: {roc_aucs.mean()}\")\n    print(f\"std: {roc_aucs.std()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load data and split into train/test\ndata = pd.read_csv('/kaggle/input/advanced-dls-spring-2021/train.csv')\nX = data[feature_cols]\ny = data[target_col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine all default data transformations into single pipeline\nfrom sklearn.pipeline import Pipeline\n\npreprocessing_pipeline = Pipeline(\n    [(\"TotalSpent\", TotalSpentTransformer()),\n     (\"SpendRatio\", FunctionTransformer(add_spend_ratio)),\n\n     (\"ClientPeriod\", FunctionTransformer(ClientPeriodTransformer)),\n     \n     (\"MonthlyAvg\", FunctionTransformer(add_monthly_avg)),\n     (\"LastMoreThanAvg\", FunctionTransformer(add_last_more_than_avg)),\n\n     (\"MonthlySpendingLog\", FunctionTransformer(add_MonthlySpendingLog)),\n     (\"TotalSpentLog\", FunctionTransformer(add_TotalSpentLog)),\n    ]\n)\n\nnum_cols = num_cols + [\"spend_ratio\"]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# IsolationForest"},{"metadata":{"trusted":true},"cell_type":"code","source":"if_pipeline = make_pipeline(\n    preprocessing_pipeline,\n    myOneHotEncoderTransformer(cat_cols),\n    IsolationForest(contamination=0.02)\n)\nif_pipeline.fit(X)\nanomaly_scores = if_pipeline.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anomaly_df = pd.DataFrame(anomaly_scores, columns=[\"anomaly\"])\n\nA = pd.concat([y, anomaly_df, X], axis=1)\nA[A[\"anomaly\"] == -1]\n\n# X=X[anomaly_scores==1]\n# y=y[anomaly_scores==1]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg_pipeline = make_pipeline(\n    preprocessing_pipeline,\n    myOneHotEncoderTransformer(cat_cols),\n    StandardScaler(),\n#     PCA(n_components=23),\n    \n    LogisticRegression(penalty=\"l2\", C=10, l1_ratio=None, random_state=2, solver=\"saga\", max_iter=10000),\n)\n\nprint_roc_auc_score(logreg_pipeline, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing logreg params\nparams = [\n    {\n#         \"logisticregression__C\": [100, 10, 1, 0.1, 0.01, 0.001],\n#         \"logisticregression__penalty\": [\"l2\", \"none\"],\n        \"logisticregression__class_weight\": [\"balanced\", None],\n#         \"logisticregression__l1_ratio\": [None]\n    },\n\n#     {\n#         \"logisticregression__C\": [100, 10, 1, 0.1, 0.01, 0.001],\n#         \"logisticregression__penalty\": [\"elasticnet\"],\n#         \"logisticregression__class_weight\": [\"balanced\", None],\n#         \"logisticregression__l1_ratio\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n#     }\n]\n\n\nlogreg_grid = GridSearchCV(logreg_pipeline, params, cv=5, verbose=4, scoring='roc_auc', refit=True, n_jobs=-1)\nlogreg_grid.fit(X, y)\nprint(logreg_grid.best_params_)\nlogreg_grid.cv_results_['mean_test_score']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_pipeline = make_pipeline(\n    preprocessing_pipeline,\n    myOneHotEncoderTransformer(cat_cols),\n    StandardScaler(),\n    GaussianNB()\n)\n\nprint_roc_auc_score(nb_pipeline, X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVC"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_pipeline = make_pipeline(\n    preprocessing_pipeline,\n    myOneHotEncoderTransformer(cat_cols),\n    StandardScaler(),\n    SVC(probability=True, C=0.5, kernel=\"poly\", gamma=\"scale\", class_weight=\"balanced\", degree=1)\n)\n\nprint_roc_auc_score(svc_pipeline, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = [{\n#                \"svc__C\": [10, 5, 1, 0.5, 0.1, 0.05, 0.01, 0.005],\n#                \"svc__kernel\": [\"linear\", \"rbf\", \"sigmoid\"],\n#                \"svc__gamma\": [\"scale\", \"auto\"],\n               \"svc__class_weight\": [\"balanced\", None],\n          },\n\n#           {\n#                \"svc__C\": [10, 5, 1, 0.5, 0.1, 0.05, 0.01, 0.005],\n#                \"svc__kernel\": [\"poly\"],\n#                \"svc__gamma\": [\"scale\", \"auto\"],\n#                \"svc__class_weight\": [\"balanced\", None],\n#                \"svc__degree\": [1, 2, 3, 4]\n#           }\n]\n\nlogreg_grid = GridSearchCV(svc_pipeline, params, cv=3, verbose=4, scoring='roc_auc', refit=True, n_jobs=-1)\nlogreg_grid.fit(X, y)\nprint(logreg_grid.best_params_)\nlogreg_grid.cv_results_['mean_test_score']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CatBoost"},{"metadata":{"id":"Fioxxlp-EEZS","trusted":true},"cell_type":"code","source":"catboost_pipeline = make_pipeline(\n    preprocessing_pipeline,\n    CatBoostClassifier(iterations=400,\n                       depth=4,\n                       learning_rate=0.05,\n                       loss_function='Logloss',\n                       verbose=False,\n                       cat_features=cat_cols,\n                       random_seed = 4,\n                       l2_leaf_reg = 40,\n                       eval_metric='AUC')\n)\n\nprint_roc_auc_score(catboost_pipeline, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \"catboostclassifier__depth\": [4, 6, 8, 10],\n#     \"catboostclassifier__learning_rate\": [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.5, 1],\n#     \"catboostclassifier__l2_leaf_reg\": [0.1, 0.5, 1, 10, 20, 40, 60],\n#     \"catboostclassifier__iterations\": [100, 200, 500],\n#     \"catboostclassifier__random_seed\": [1, 2, 3, 4, 5, 6, 7, 8, 9], # test random seed just for fun =)\n}\n\ncb_grid = GridSearchCV(catboost_pipeline, params, cv=5, verbose=4, scoring='roc_auc', refit=True, n_jobs=-1)\ncb_grid.fit(X, y)\nprint(cb_grid.best_params_)\ncb_grid.cv_results_['mean_test_score']","execution_count":null,"outputs":[]},{"metadata":{"id":"dDMXbvNZEEZV"},"cell_type":"markdown","source":"# Предсказания"},{"metadata":{},"cell_type":"markdown","source":"# XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_pipeline = make_pipeline(\n    preprocessing_pipeline,\n    myOneHotEncoderTransformer(cat_cols),\n    XGBClassifier(n_jobs=-1,\n                  use_label_encoder=False,\n                  learning_rate=0.1,\n                  max_depth=1,\n                  n_estimators=200)\n\n)\n\nprint_roc_auc_score(xgb_pipeline, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n#     \"xgbclassifier__n_estimators\": [50, 100, 200, 500, 1000],\n#     \"xgbclassifier__max_depth\": [1, 2, 4, 6, 8, 10],\n#     \"xgbclassifier__learning_rate\": [0.001, 0.1, 1, 10],\n\n    \"xgbclassifier__max_depth\": [1, 2],\n}\n\ndt_grid = GridSearchCV(xgb_pipeline, params, cv=5, verbose=4, scoring='roc_auc', refit=True, n_jobs=-1)\ndt_grid.fit(X, y)\nprint(dt_grid.best_params_)\ndt_grid.cv_results_['mean_test_score']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DecisionTreeClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree_pipeline = make_pipeline(\n    preprocessing_pipeline,\n    myOneHotEncoderTransformer(cat_cols),\n    DecisionTreeClassifier(min_samples_leaf=15, max_depth=5, random_state=2)\n)\n\nprint_roc_auc_score(dtree_pipeline, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \"decisiontreeclassifier__min_samples_leaf\": [1, 5, 10, 15, 20],\n    \"decisiontreeclassifier__max_depth\": [5, 10, 15, 20, 100],\n}\n\ndt_grid = GridSearchCV(dtree_pipeline, params, cv=5, verbose=4, scoring='roc_auc', refit=True, n_jobs=-1)\ndt_grid.fit(X, y)\nprint(dt_grid.best_params_)\ndt_grid.cv_results_['mean_test_score']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RandomForestClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_pipeline = make_pipeline(\n    preprocessing_pipeline,\n    myOneHotEncoderTransformer(cat_cols),\n#     PCA(n_components=23),\n    RandomForestClassifier(n_estimators=200, min_samples_leaf=20, max_depth=100, random_state=2,\n                           class_weight=\"balanced\",\n                           criterion=\"entropy\"\n                          )\n)\n\nprint_roc_auc_score(random_forest_pipeline, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n#     \"randomforestclassifier__n_estimators\": [20, 50, 100, 200, 500],\n#     \"randomforestclassifier__min_samples_leaf\": [1, 5, 10, 15, 20, 25, 30],\n#     \"randomforestclassifier__max_depth\": [5, 10, 15, 20, 100],\n    \"randomforestclassifier__criterion\": [\"gini\", \"entropy\"],\n#     \"randomforestclassifier__class_weight\" : [\"balanced\", \"balanced_subsample\", None]\n}\n\nrf_grid = GridSearchCV(random_forest_pipeline, params, cv=5, verbose=4, scoring='roc_auc', refit=True, n_jobs=-1)\nrf_grid.fit(X, y)\nprint(rf_grid.best_params_)\nrf_grid.cv_results_['mean_test_score']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNeighborsClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_pipeline = make_pipeline(\n    preprocessing_pipeline,\n    myOneHotEncoderTransformer(cat_cols),\n    PCA(n_components=23),\n    StandardScaler(),\n    \n    KNeighborsClassifier(n_neighbors=40, weights=\"uniform\", p=1, metric='manhattan')\n)\n\nprint_roc_auc_score(knn_pipeline, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n#     \"kneighborsclassifier__n_neighbors\": [1,2, 4, 6, 8, 10, 15, 20, 30, 40, 50, 70, 100],\n    \"kneighborsclassifier__weights\": [\"uniform\", \"distance\"],\n#     \"kneighborsclassifier__p\": [1, 2],\n#     \"kneighborsclassifier__metric\": [\"euclidean\", \"manhattan\", \"chebyshev\", \"minkowski\"]\n}\n\nrf_grid = GridSearchCV(knn_pipeline, params, cv=5, verbose=4, scoring='roc_auc', refit=True, n_jobs=-1)\nrf_grid.fit(X, y)\nprint(rf_grid.best_params_)\nrf_grid.cv_results_['mean_test_score']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stacking"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = [(\"logreg\", logreg_pipeline),\n              (\"dtree\", dtree_pipeline),\n#               (\"rf\", random_forest_pipeline),\n#               (\"cb\", catboost_pipeline),\n#               (\"knn\", knn_pipeline),\n              (\"nb\", nb_pipeline),\n#               (\"svc\", svc_pipeline),\n#               (\"xgb\", xgb_pipeline),\n             ]\nfinal_estimator = LogisticRegression(max_iter=10000, solver=\"saga\", C=1)\n\nstacking = StackingClassifier(estimators=estimators, \n                              final_estimator=final_estimator,\n                              cv=5,\n                              stack_method=\"predict_proba\",\n                              passthrough=False)\n\nprint_roc_auc_score(stacking, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \"logreg__logisticregression__C\": [100, 10, 1, 0.1, 0.01, 0.001],\n#     \"logreg__logisticregression__penalty\": [\"l1\", \"l2\", \"elasticnet\", \"none\"],\n#     \"logreg__logisticregression__l1_ratio\": [0.1, 0.5, 0.9],\n\n    \"dtree__decisiontreeclassifier__min_samples_leaf\": [1, 5, 10, 20],\n#     \"dtree__decisiontreeclassifier__max_depth\": [5, 10, 15, 20, 100],\n\n#     \"rf__randomforestclassifier__n_estimators\": [50, 100, 200, 500, 1000],\n#     \"rf__randomforestclassifier__min_samples_leaf\": [1, 5, 10, 20, 30],\n#     \"rf__randomforestclassifier__max_depth\": [5, 10, 15, 20, 100],\n#     \"rf__randomforestclassifier__criterion\": [\"gini\", \"entropy\"],\n#     \"rf__randomforestclassifier__class_weight\" : [\"balanced\", \"balanced_subsample\", None],\n\n#     (n_estimators=200, min_samples_leaf=30, max_depth=15, \n#     \"rf__randomforestclassifier__n_estimators\": [100, 200, 500, 1000],\n#     \"rf__randomforestclassifier__min_samples_leaf\": [1, 10, 30],\n#     \"rf__randomforestclassifier__max_depth\": [5, 10, None],\n#     \"rf__randomforestclassifier__criterion\": [\"gini\"],\n#     \"rf__randomforestclassifier__class_weight\" : [\"balanced\", \"balanced_subsample\", None],\n\n    \n#     \"cb__catboostclassifier__depth\": [2, 4, 6, 8, 10],\n#     \"cb__catboostclassifier__learning_rate\": [0.05, 0.1, 0.5, 1],\n#     \"cb__catboostclassifier__l2_leaf_reg\": [0.1, 1, 10, 30, 60],\n#     \"cb__catboostclassifier__iterations\": [50, 100, 200, 500, 1000],\n\n#     \"knn__kneighborsclassifier__n_neighbors\": [1, 2, 3, 4, 5, 10, 20, 30, 50],\n#     \"knn__kneighborsclassifier__weights\": [\"uniform\", \"distance\"],\n#     \"knn__kneighborsclassifier__p\": [1, 2],\n#     \"knn__kneighborsclassifier__metric\": [\"euclidean\", \"manhattan\", \"chebyshev\", \"minkowski\"],\n\n    \n#     \"svc__svc__C\": [1, 0.1, 0.01, 0.001],\n#     \"svc__svc__kernel\": [\"linear\", \"rbf\", \"sigmoid\", \"poly\"],\n#     \"svc__svc__class_weight\": [\"balanced\", None],\n\n#     \"xgb__xgbclassifier__n_estimators\": [50, 100, 200, 500, 1000],\n#     \"xgb__xgbclassifier__max_depth\": [2, 4, 6, 8, 10],\n#     \"xgb__xgbclassifier__learning_rate\": [0.05, 0.1, 1, 10],\n}\n\n\nrf_grid = GridSearchCV(stacking, params, cv=3, verbose=4, scoring='roc_auc', refit=True, n_jobs=-1)\n# rf_grid = RandomizedSearchCV(stacking, params, cv=5, verbose=4, scoring='roc_auc', refit=True, n_jobs=-1)\nrf_grid.fit(X, y)\nprint(rf_grid.best_params_)\nrf_grid.cv_results_['mean_test_score']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Manual fit stacking"},{"metadata":{"trusted":true},"cell_type":"code","source":"class StackBaseTransformer(BaseEstimator, TransformerMixin):\n    estimators = [(\"logreg\", logreg_pipeline),\n                  (\"dtree\", dtree_pipeline),\n                  (\"rf\", random_forest_pipeline),\n                  (\"cb\", catboost_pipeline),\n                  (\"knn\", knn_pipeline),\n                  (\"nb\", nb_pipeline),\n                  (\"svc\", svc_pipeline),\n                  (\"xgb\", xgb_pipeline),\n                 ]\n    \n    def fit(self, X, y=None):\n        for name, model in self.estimators:\n            model.fit(X, y)\n\n    def transform(self, X):\n        newX = pd.DataFrame()\n        for name, model in self.estimators:\n            newX[name] = model.predict_proba(X)[:, 1].round(2)\n        return newX\n\nSBT = StackBaseTransformer()\nSBT.fit(X, y)\nX_sbt = SBT.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([y, X_sbt, X], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(X_sbt.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# choose model for private data"},{"metadata":{"id":"FfSufx0CEEZZ","trusted":true},"cell_type":"code","source":"best_model = stacking\nbest_model.fit(X, y) # refit model on whole data\n\nX_private = pd.read_csv('/kaggle/input/advanced-dls-spring-2021/test.csv')\nsubmission = pd.read_csv('/kaggle/input/advanced-dls-spring-2021/submission.csv')\nsubmission['Churn'] = best_model.predict_proba(X_private)[:, 1]\nsubmission.to_csv('./my_submission.csv', index=False, columns=[\"Id\", \"Churn\"])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}